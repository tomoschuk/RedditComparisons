{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit thread comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my favorite things about reddit is the diversity of communities. Some of the weirdest communties in the world gather to express their opinions on things. What I find particularly interesting about this fact, is that people will gather in the same place and have totally different opinions based on the communties their in. For an obvious example, some of the same topics will work their way through r/liberal and r/conservative: the recent election, current administration policy etc. Suffice it to say: their opinions differ.\n",
    "\n",
    "I wanted a way to quantify the difference in sentiment between a topic in two communities, so I've written this script using a fairly innocuous example: Marvel/Netflix's Iron Fist as discussed by a general population of television watchers vs. Marvel fans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iron Fist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netflix's Marvel shows have been wildly popular - Daredevil, Jessica Jones and Luke Cage have all scored at least 75 on Metacritic. The most recent addition...has not done so well. [Metacritic gives Iron Fist a 37.](www.metacritic.com/tv/marvels-iron-fist) Interestingly, users seem to disagree with critics. Many more comments on metacritic seem to have positive reviews. Is it that critics are being unusually harsh? Are fans unusually forgiving? My based-on-nothing hypothesis is that the critics and fans have different interests. Critics represent a more general audience that will care about things like dialogue, while fans are more interested in how true it is to the source material. It struck me as an interesting exercise to see if I could quantify this disparity (using sentiment classification)...and then get down to the root of its cause (using topic extraction). \n",
    "\n",
    "Multiple subreddits have threads discussing Iron Fist, [r/television](https://www.reddit.com/r/television/comments/5zvzlz/iron_fist_series_premiere_discussion/?limit=500) and [r/marvelstudios](https://www.reddit.com/r/marvelstudios/comments/5zva2a/the_official_iron_fist_discussion_ultrathread/?sort=top&limit=500) being among them. I'm going analyze all of the top level comments of these communities, treating r/tv commenters as a general audience (critics theoretically voice this community's opinions) and r/marvelstudios as fans.\n",
    "\n",
    "So to do this I will:\n",
    "1. [Scrape and process the data, including parsing, lemmatization, and stop word removal.](#section1)\n",
    "2. [Train a Linear SVC and Na√Øve Bayes Sentiment Classification model on a training set of movie reviews that are already classified as positive or negative.](#section2)\n",
    "3. [Use that model to classify and visualize sentiment from the two communities.](#section3)\n",
    "4. [Extract topics from the different communities to try to understand who's talking about what.](#section4)\n",
    "\n",
    "[Jump to the conclusion!](#conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import spacy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def is_punct_space(token):\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#Add some custom stopwords\n",
    "#These need to change with the threads in question\n",
    "spacy.en.language_data.STOP_WORDS.update(['-PRON-','think','feel','like','episode',\"'s\", 'season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Scrape the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this you need your own Reddit account, and you need to use it to get a client_id and client_secret. Mine are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 4.5.0 of praw is outdated. Version 4.5.1 was released Monday May 08, 2017.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(user_agent='Comment Extraction (by /u/??)',\n",
    "                     client_id='??', client_secret=\"??\")\n",
    "\n",
    "tv = reddit.submission(url='https://www.reddit.com/r/television/comments/5zvzlz/iron_fist_series_premiere_discussion/?limit=500')\n",
    "fans = reddit.submission(url='https://www.reddit.com/r/marvelstudios/comments/5zva2a/the_official_iron_fist_discussion_ultrathread/?sort=top&limit=500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv_comments = []\n",
    "tv.comments.replace_more(limit=0)\n",
    "for top_level_comment in tv.comments:\n",
    "    tv_comments.append(top_level_comment.body)\n",
    "    \n",
    "marvel_comments = []\n",
    "fans.comments.replace_more(limit=0)\n",
    "for top_level_comment in fans.comments:\n",
    "    marvel_comments.append(top_level_comment.body)\n",
    "\n",
    "#Create dataframes\n",
    "all_comments = pd.DataFrame({'raw': tv_comments,'source':'tv'})\n",
    "fans_comments = pd.DataFrame({'raw': marvel_comments, 'source':'fans'})\n",
    "\n",
    "#Remove any comments that are longer than one standard deviation above the median. \n",
    "#I love a good internet rant, but I don't want ones that are too long influencing the topic extraction too much.\n",
    "median_fans_comment_length = np.median(fans_comments['raw'].map(len))\n",
    "median_tv_comment_length = np.median(all_comments['raw'].map(len))\n",
    "std_fans_comment_length = np.std(fans_comments['raw'].map(len))\n",
    "std_tv_comment_length = np.std(all_comments['raw'].map(len))\n",
    "fans_comments = fans_comments[fans_comments['raw'].map(len) < (median_fans_comment_length+std_fans_comment_length)]\n",
    "all_comments = all_comments[all_comments['raw'].map(len) < (median_tv_comment_length+std_tv_comment_length)]\n",
    "\n",
    "#Figure out which of the two datasets has more comments, then reduce the bigger one to be the same size as the smaller\n",
    "if len(fans_comments) > len(all_comments):\n",
    "    fans_comments = fans_comments.head(len(all_comments))\n",
    "else:\n",
    "    all_comments = all_comments.head(len(head_comments))\n",
    "\n",
    "#Merge to one dataframe\n",
    "all_comments = all_comments.append(fans_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(data):\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.raw = [r.replace('\\n', '') for r in data.raw]\n",
    "    \n",
    "    #Parse and lemmatize the data\n",
    "    data['parsed'] = [ nlp(x) for x in data.raw]\n",
    "    data['lemmatized'] = [[token.lemma_ for token in x\n",
    "                            if not is_punct_space(token)] \n",
    "                  for x in data.parsed]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    data['cleaned_comments'] = [[term for term in x\n",
    "                                if not term in spacy.en.language_data.STOP_WORDS]\n",
    "                               for x in data.lemmatized]\n",
    "    #Collapse into one string rather than list\n",
    "    data.cleaned_comments = [' '.join(x) for x in data.cleaned_comments]\n",
    "    \n",
    "    #Create a couple bigrames for names - we don't want the model to confused Jessica Jones and Finn Jones\n",
    "    data.cleaned_comments = [r.replace('jessica jones', 'jessica_jones') for r in data.cleaned_comments]\n",
    "    data.cleaned_comments = [r.replace('finn jones', 'finn_jones') for r in data.cleaned_comments]\n",
    "    data.cleaned_comments = [r.replace('luke cage', 'luke_cage') for r in data.cleaned_comments]\n",
    "    data.cleaned_comments = [r.replace('iron fist', 'iron_fist') for r in data.cleaned_comments]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bigrams were chosen by hand, so this would need to change for any comparison of two different threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>source</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I actually like it, but my biggest problem so ...</td>\n",
       "      <td>tv</td>\n",
       "      <td>(I, actually, like, it, ,, but, my, biggest, p...</td>\n",
       "      <td>[-PRON-, actually, like, -PRON-, but, -PRON-, ...</td>\n",
       "      <td>actually big problem far soon claire find hand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw source  \\\n",
       "0  I actually like it, but my biggest problem so ...     tv   \n",
       "\n",
       "                                              parsed  \\\n",
       "0  (I, actually, like, it, ,, but, my, biggest, p...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [-PRON-, actually, like, -PRON-, but, -PRON-, ...   \n",
       "\n",
       "                                    cleaned_comments  \n",
       "0  actually big problem far soon claire find hand...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = nlp_pipeline(all_comments)\n",
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to add a new column to show the progression of the pipeline, rather than transforming to only one list. Parsing turns it into a list of strings, lemmatizing it lowercases it, and identifies parts of speech. The cleaned_comments is the column we use for everything in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Train a Sentiment Classification model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have taken a corpus of movie reviews ([found here](http://www.cs.cornell.edu/people/pabo/movie-review-data/)) that have already been classified as negative or positive. This will serve as the training material for our sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set path to positive and negative folders (my information removed)\n",
    "ppath = '??'\n",
    "npath = '??'\n",
    "\n",
    "#Open negative files and put them into a pandas dataframe\n",
    "os.chdir(npath)\n",
    "ndat = []\n",
    "ffiles = [f for f in os.listdir(npath) if os.path.isfile(f)]\n",
    "for f in ffiles:\n",
    "  with open (f, \"r\") as myfile:\n",
    "    ndat.append(myfile.read())\n",
    "\n",
    "reviews = pd.DataFrame({'raw':ndat,'sentiment':'negative'})\n",
    "\n",
    "#Positive files into a separate dataframe then merge the two\n",
    "os.chdir(ppath)\n",
    "pdat = []\n",
    "pfiles = [f for f in os.listdir(ppath) if os.path.isfile(f)]\n",
    "for f in pfiles:\n",
    "  with open (f, \"r\") as myfile:\n",
    "    pdat.append(myfile.read())\n",
    "pos_reviews = pd.DataFrame({'raw':pdat,'sentiment':'positive'})\n",
    "\n",
    "#Merge\n",
    "reviews = reviews.append(pos_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>parsed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>negative</td>\n",
       "      <td>(plot, :, two, teen, couples, go, to, a, churc...</td>\n",
       "      <td>[plot, two, teen, couple, go, to, a, church, p...</td>\n",
       "      <td>plot teen couple church party drink drive acci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 raw sentiment  \\\n",
       "0  plot : two teen couples go to a church party ,...  negative   \n",
       "\n",
       "                                              parsed  \\\n",
       "0  (plot, :, two, teen, couples, go, to, a, churc...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [plot, two, teen, couple, go, to, a, church, p...   \n",
       "\n",
       "                                    cleaned_comments  \n",
       "0  plot teen couple church party drink drive acci...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = nlp_pipeline(reviews)\n",
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first kind of model I want to try is a basic LinearSVC, which will try to classify based simply on matching vectors. Let's also see what that looks like in a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Turn the comments into vectors, train the model, and return the dataframe with a sentiment column\n",
    "def classify(traindata,testdata, classification_method):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_vec = vectorizer.fit_transform(traindata.cleaned_comments)\n",
    "    test_vec = vectorizer.transform(testdata.cleaned_comments)\n",
    "    classifier = classification_method\n",
    "    classifier.fit(train_vec, traindata.sentiment)\n",
    "    testdata['sentiment'] = classifier.predict(test_vec)\n",
    "    return testdata\n",
    "\n",
    "#To validate the model, this splits the training data, and prints a cross-validated accuracy\n",
    "def print_model_acc(traindata, classification_method):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(traindata.cleaned_comments,\n",
    "                                                        traindata.sentiment, test_size=0.2, random_state=4)\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_vec = vectorizer.fit_transform(X_train)\n",
    "    test_vec = vectorizer.transform(X_test)\n",
    "    classifier = classification_method\n",
    "    classifier.fit(train_vec, Y_train)\n",
    "    print(classifier.score(test_vec, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "print_model_acc(train_data, LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that's not a bad classification accuracy. Now let's apply it to our test set and see what it looks like in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEUCAYAAACBNEVpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh8IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x164404ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = classify(train_data,test_data, LinearSVC())\n",
    "\n",
    "#Plot the results using seaborn\n",
    "agg=test_data.groupby(['sentiment','source']).size().reset_index()\n",
    "agg.columns=['sentiment','source','size']\n",
    "fig1 = sns.factorplot(x=\"source\", y=\"size\", hue=\"sentiment\", data=agg, kind=\"bar\")\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the results, let me break down the graph a bit. On the y-axis we simply see the number of items that got sorted into each category. On the x-axis is the source material (so did these reddit comments come from fans or tv). \n",
    "\n",
    "Our model was pretty good at identifying items in the train set, but doesn't really seem to capture a difference here. I think it's worth it to also try a Multinomial Naive Bayes, which will assume independence of all the words being in a given comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "print_model_acc(train_data, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAEUCAYAAACBNEVpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhKCNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5f5f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = classify(train_data,test_data, MultinomialNB())\n",
    "\n",
    "agg2=test_data.groupby(['sentiment','source']).size().reset_index()\n",
    "agg2.columns=['sentiment','source','size']\n",
    "fig1 = sns.factorplot(x=\"source\", y=\"size\", hue=\"sentiment\", data=agg2, kind=\"bar\")\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That captures way more nuance in the data than the LinearSVC and doesn't sacrifice any of the model's accuracy. This is probably because a Naive Bayes classifier has much more lax assumptions about the independence of the features of the model.\n",
    "\n",
    "Just to be sure though let's look at a random comment for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative example:\n",
      "1    The dialogue and acting improves as it goes, but fuck it's slow. Every episode I've seen is at lest 15-20 minutes too long. \n",
      "Name: raw, dtype: object\n",
      "\n",
      "Positive example:\n",
      "276    Just finished watching the finale, really don't think it deserves the hate it's getting here. I thought it was pretty good; started off slow and not enough use of the Iron Fist, ended a bit weak but I thought the fight scenes were really cool and enjoyed the story (the bits with the Hand, anyway).\n",
      "Name: raw, dtype: object\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print(\"Negative example:\")\n",
    "print(test_data.loc[test_data.sentiment=='negative','raw'].sample(1))\n",
    "print()\n",
    "print(\"Positive example:\")\n",
    "print(test_data.loc[test_data.sentiment=='positive','raw'].sample(1))\n",
    "print(\"***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry for the profanity...but that's internet commenting for you.\n",
    "\n",
    "Anyway, a decent classification difference! It's clear that the any one comment isn't going to be wholly positive or negative. These comments are clearly nuanced and the Multinomial Naive Bayes model seems to do an better job of catching general sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Extracting and comparing topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now that we've looked at the sentiment, it's time to try to extract some topics. The overarching goal of this section is to see if different communities are talking about different things. To do that we are going to define two similar functions that vectorize the comments and fit them to an LDA model, which, given a number of proposed topics, will associates words with each topic in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def source_topic_extraction(data, source, n_features, n_topics):\n",
    "    #Vectorize comments\n",
    "    vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    topic_vec = vectorizer.fit_transform(test_data[test_data.source == source].cleaned_comments)\n",
    "    #Add and fit them to an LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_topics = n_topics, learning_method = 'online')\n",
    "    lda_model.fit(topic_vec)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return lda_model, feature_names\n",
    "\n",
    "def sent_topic_extraction(data, sent, n_features, n_topics):\n",
    "    #Nearly identical to the model above, but filters based on SENTIMENT instead of SOURCE\n",
    "    vectorizer = CountVectorizer(min_df=5, max_features=n_features,stop_words='english')\n",
    "    topic_vec = vectorizer.fit_transform(test_data[test_data.sentiment == sent].cleaned_comments)\n",
    "    lda_model = LatentDirichletAllocation(n_topics = n_topics, learning_method = 'online')\n",
    "    lda_model.fit(topic_vec)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return lda_model, feature_names\n",
    "\n",
    "#Given two topic models, prints top n words for each topic\n",
    "def compare_topics(model1, feature_names1, model2, feature_names2, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model1.components_):\n",
    "        print(\"Model 1 topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names1[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    for topic_idx, topic in enumerate(model2.components_):\n",
    "        print(\"Model 2 topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names2[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build two models, one representing fans and the other representing tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 topic #0:\n",
      "danny iron_fist fight series scene\n",
      "Model 1 topic #1:\n",
      "good far character enjoy watch\n",
      "\n",
      "Model 2 topic #0:\n",
      "watch bad good time review\n",
      "Model 2 topic #1:\n",
      "good daredevil fight luke_cage scene\n"
     ]
    }
   ],
   "source": [
    "fanmodel, fannames = source_topic_extraction(test_data, 'fans', 50, 2)\n",
    "tvmodel, tvnames = source_topic_extraction(test_data, 'tv', 50, 2)\n",
    "compare_topics(fanmodel, fannames, tvmodel, tvnames, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is that of the fans, and the second is that of the tv watchers.\n",
    "\n",
    "I used relatively few topics because many of the comments are quite short, and too many comments with too many features would be kind of an overextension of our data. Given that, there are some obvious differences in topics. Topics from fans discuss the characters and the fighting, while TV posters compare Iron Fist to Daredevil and Luke Cage.\n",
    "\n",
    "I think, to some degree, this supports the idea that these two communities are focusing on different things, with the most obvious difference being that the tv posters immediately compare Iron Fist to the other Netflix shows.\n",
    "\n",
    "To take it one step further, I'll do two topic analyses, separated by sentiment rather than community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 topic #0:\n",
      "great danny good iron_fist enjoy\n",
      "Model 1 topic #1:\n",
      "fight marvel come choreography good\n",
      "\n",
      "Model 2 topic #0:\n",
      "bad daredevil enjoy netflix far\n",
      "Model 2 topic #1:\n",
      "good danny fight character scene\n"
     ]
    }
   ],
   "source": [
    "posmodel, posnames = sent_topic_extraction(test_data, 'positive', 50, 2)\n",
    "negmodel, negnames = sent_topic_extraction(test_data, 'negative', 50, 2)\n",
    "compare_topics(posmodel, posnames, negmodel, negnames, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Definitely take these results with a grain of salt. Almost all of the positive comments come from the fans community, so it isn't as nice a split as the last analysis.\n",
    "\n",
    "That said, there are a few trends that differ. The positive comments seem to mention the fight choreography, while the negative comments talk about the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conc'></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have we learned? \n",
    "\n",
    "1. Fans clearly have more mixed opinions than the more general tv audience. \n",
    "2. The topics these fans care about vary, if only somewhat.\n",
    "\n",
    "\n",
    "I was altogether pretty happy with this code. Using SpaCy instead of nltk was a bit new, but was pretty simple, and will likely do a good job of scaling up if I get more comments. \n",
    "\n",
    "\n",
    "Some drawbacks/limitations:\n",
    "1. Reddit comments aren't typically very long...longer reviews from another site would probably contribute better to the models, though there are likely to be fewer reviews.\n",
    "2. This code doesn't capture reddit's upvote/downvote system. In an ideal world, I'd like to be able to weigh some comments as more influential, based on their score, to reflect that many people who come to the comment section simply agree and move on. \n",
    "3. The code needs manual bigrams for names, which is somewhat limiting to its generalizability. In the next iteration I want to add code that finds common bigrams and combines them.\n",
    "\n",
    "\n",
    "This code is otherwise pretty flexible and should be usable to compare the sentiment and topics of any two reddit threads given two links (and some edited stopwords). Iron Fist was a nice innocent example because there was a strong precedent for differing opinions between communities, but I think this would work just as well for posts from different political communities, religious communities etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
